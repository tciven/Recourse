{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Text Generation\n",
    "\n",
    "One interesting problem in artificial intelligence is the . It is one of the oldest questions in computer intelligence, posed formally in the [Turing Test](https://en.wikipedia.org/wiki/Turing_test), which tests whether not a computer can be nidistinguishable from a human in a text only chat. This model however stays far away from such a task, instead attempts to generate new recourse from a single speaker based on a corpus of recorded speech or writing.\n",
    "\n",
    "We work here with deterministic, probability based models. Building from low to high complexity, we will create and evaluate several different types of models all based on [Markov Chains](https://en.wikipedia.org/wiki/Markov_chain).\n",
    "\n",
    "Markov Chains describe a \n",
    "Here, we will define a state to be a word that we have just generated, and this state will have a transition proabbilities based on every word that came after it in the provided corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recourse\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example I will be using speeches given by Dr. Martin Luther King Jr. I selected him not only for his imprtance but also his characteristic style of speeh, which I wanted to see how well this model could capture. The test is from Richten Park Public LIbrary, and the download for the original document can be found [here](wmasd.ss7.sharpschool.com/common/pages/UserFile.aspx?fileId=8373388).\n",
    "\n",
    "A text file is in this repository of a cleaner version of this pdf, however it still contains a lot of puncuation and requires just a little more prep before it is ready to go into a model. We need an all lower case list of the words in order without puncuation (discussion of the impact of this on the model will be discussed later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open(\"king.txt\",\"r\")\n",
    "text = corpus.read() \n",
    "corpus.close()\n",
    "text = re.sub(\"[^A-Za-z0-9 ]+\", \"\", text)\n",
    "text = text.split()\n",
    "text = [word.lower() for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr',\n",
       " 'chairman',\n",
       " 'distinguished',\n",
       " 'platform',\n",
       " 'associates',\n",
       " 'fellow',\n",
       " 'americans',\n",
       " 'three',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'the',\n",
       " 'supreme',\n",
       " 'court',\n",
       " 'of',\n",
       " 'this',\n",
       " 'nation',\n",
       " 'rendered',\n",
       " 'in',\n",
       " 'simple',\n",
       " 'eloquent']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let us take just about the simplest way to generate text based on a corpus we could think up, and have each generated word simply be randomly sampled from the corpus. This can be accomplished with the ```zero_model``` class within the recourse module. First we will build the model, and then use the function ```gen``` to create new sentences of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zero = recourse.zero_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'path thinking get of injected one your in for themselves that in yeah deep bard and war because in church nothing of of has im this they 50 that nations as contemplation their letter now former us every so plucked'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_zero.gen(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appraoch performs about as well as we could expect. While the vocabulary seems right, all together the language is unsurprisingly garbled.\n",
    "\n",
    "To improve on this, let's build a model based on Markov Chains.\n",
    "\n",
    "To build our model, we will record the order of the text, meaning counting how often words follow other words. What we will end up with is a list of distributions, one for each unique word in the text. The distributions will tell us what words tend to follow the current word. \n",
    "\n",
    "The function one_word_model takes a body of text (formatted in one string) and returns a model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one = recourse.one_word_model(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have these distributions encased in our model object, we are ready to generate! \n",
    "\n",
    "We can randomly choose a starting \"seed\" word, and then use its distribution in our matrix to generate the next word, use that next word's distribution to generate a third, and so on. The gen function in every model object will do just that. All it needs is the amount of words desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "everywhere longer guardians nation night eloquent latin plato me language gone saying permanently on court understanding language daybreak night language dialogues which peasants came night eloquent language think despitefully university men smoother more stressing court\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'be relevant marvel governors dared freedom stressing court language dialogues which proof permanently on court language gone saying night nation night nation symbolic above name case nation rendered eloquent language beautifully eloquent trustful vote capitulating'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_one.gen(35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the language becomes a little more free flowing, but is still nearly unintelligable. We can improve on this by building a new model that does not just consider what follows every unique word, but what follows every unique pair of words. This lets it have a little more context in generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two = recourse.two_word_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be able to do that and i insisted that i had to sit down and i have a dream that one day even the race problem let us realize that we have had a fellow in'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_two.gen(35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b3d8a8f0a76e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfirstlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mfirstlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirstlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = 737\n",
    "reps = 100000\n",
    "\n",
    "firstlist = []\n",
    "\n",
    "for i in range(reps):\n",
    "    x = x**1.00004\n",
    "    firstlist.append(int(str(x)[0]))\n",
    "\n",
    "firstlist = np.array(firstlist)\n",
    "print(i)\n",
    "for i in range(1,10):\n",
    "    print(i, sum(firstlist == i) / reps)\n",
    "    \n",
    "print(\"Final x value:\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Potential Improvements\n",
    "While these models have their moments, there is room for a lot of improvement. Chief amoung these is the lack of any way to define sentences, it just goes on until it reaches the spcified number of words. I think a more intuitive way to build speech would be instead of words, ask the model for a certain number of sentences and let it determine their length.\n",
    "\n",
    "1. Determine the distribution of sentence lengths in the corpus and sample from that distribution to decide where to place commas\n",
    "2. Keep the words that end sentences as denoted by ending in a \".\" distinct from the same words that does not. For example \"things\" would be a different word in the model, and have different movement probabilites than \"things.\" \n",
    "\n",
    "The first option is computationally very cheapbut would add very little to the quality of the generator rhetoric. It does essentially the same operation and sprinkles in a couple of periods for flair. \n",
    "\n",
    "The second option, while requiring more thought in implementation, would guarantee that every first/last word in a generated sentence began/ended one sentence in the text. This would add coherence both within and between sentences. It is however exposed to more situations where the next geenrated word would be deterministic (only one time that word ended a sentence in the text). Also, there is the chance that some sentences can become very stuck in words that do not typically end sentences, and they can become unnaturally long. Additionally, because the size of the p matrices is the number of unique words (/word pairs) squared, significant increases beyond the number of unique words here (44448) (what a great number) can cause serious issues with storage of the matrix (10,000 unique words would be 100 million floating point numbers to store, and 800+ megabytes of RAM to store the model!!).\n",
    "\n",
    "This brings us to the last thought, which is that this problem of model storage generalizes to all of these markov models no matter what type of system is used. One area to explore is the storeage of sparse matrices. Out of all the unique words, only very few ever appear next to each other, and there could be massive gains made by reducing the load of the build models by utilizing something like a dictionary of keys or a coordinate list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
